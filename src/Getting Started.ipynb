{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from t3_model import T3\n",
    "from data_generator import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill in the user parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data input file (must be CSV)\n",
    "INPUT_DATA_FILENAME = \"sample-data.csv\"\n",
    "\n",
    "# names of the input data columns as they appear in the CSV\n",
    "FEATURE_COLS = ['PM1', 'PM2_5', 'PM10', 'Temperature', 'Humidity', 'RED', 'NOX'] \n",
    "\n",
    "# names of the target column[s] as they appear in the CSV\n",
    "TARGET_COLS = ['OZONE']\n",
    "\n",
    "# function to preprocess the data before it is converted into samples. \n",
    "#  Should take a Pandas DataFrame and return a new DataFrame\n",
    "PREPROCESSOR = None\n",
    "\n",
    "# function to normalize the dataset. Default is MinMaxScaler\n",
    "SCALER = MinMaxScaler()\n",
    "\n",
    "# number of rows per sample (the temporal window)\n",
    "WINDOW = 100\n",
    "\n",
    "# Periodicity, defined in seconds. Used to make sure there aren't any breaks in the sample \n",
    "PERIODICITY = 1 * 3600\n",
    "\n",
    "# Whether to use sample renormalization (concatenate features renormalized for the sample window)\n",
    "SAMPLE_RENORMALIZATION = True\n",
    "\n",
    "# The renormalization kernel to use if SAMPLE_RENORMALIZATION is True\n",
    "SAMPLE_RENORM_KERNEL = MinMaxScaler\n",
    "\n",
    "# Training batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Length of the embedding vector\n",
    "D_MODEL = 512\n",
    "\n",
    "# Number of neurons in the feed-forward network\n",
    "DFF = D_MODEL * 4\n",
    "\n",
    "# Number of Multi-Attention Heads\n",
    "NUM_HEADS = 1\n",
    "\n",
    "# Width of the head\n",
    "HEAD_SIZE = 128\n",
    "\n",
    "# Number of Multi-Attention layers in series\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "# Dropout rate\n",
    "DROPOUT_RATE = 0.8\n",
    "\n",
    "# Number of epochs to train over\n",
    "NUM_EPOCHS = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Data Pool \n",
    "\n",
    "Your data should be stored in a `.csv` file with the following properties:\n",
    "\n",
    "- The first row are headings (timestamp, pm2.5, o3)\n",
    "- The first column are valid timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 9112 from 19328 rows of data. 10116 samples were skipped due to noncontinuous data\n",
      "Creating the training dataset...\n",
      "Creating the testing dataset...\n",
      "Number of training samples: 199\n",
      "Number of testing samples:  85\n"
     ]
    }
   ],
   "source": [
    "# Read in the data to a Pandas DataFrame\n",
    "data = pd.read_csv(INPUT_DATA_FILENAME, index_col=0, parse_dates=True)\n",
    "\n",
    "# Preprocess the data if user supplied a function\n",
    "if PREPROCESSOR:\n",
    "    data = PREPROCESSOR(data)\n",
    "    \n",
    "# Normalize each data column to the appropriate range\n",
    "data.loc[:, FEATURE_COLS + TARGET_COLS] = SCALER.fit_transform(data.loc[:, FEATURE_COLS + TARGET_COLS].values)\n",
    "SCALER.feature_cols = FEATURE_COLS\n",
    "SCALER.target_cols = TARGET_COLS\n",
    "\n",
    "# Create the data pool\n",
    "SAMPLES = []\n",
    "expected_timespan = (WINDOW - 1) * PERIODICITY\n",
    "num_skipped = 0\n",
    "for i in range(len(data) - WINDOW):\n",
    "    \n",
    "    print(f'Parsed {i}/{len(data)} samples...', end='\\r')\n",
    "\n",
    "    x = data.iloc[i:i+WINDOW].copy()\n",
    "\n",
    "    sample_timespan = int((x.iloc[-1].name - x.iloc[0].name).total_seconds())\n",
    "\n",
    "    if sample_timespan != expected_timespan:\n",
    "        num_skipped += 1\n",
    "        continue\n",
    "        \n",
    "    if SAMPLE_RENORMALIZATION:\n",
    "        x2 = x.loc[:, FEATURE_COLS].copy()\n",
    "        x2.loc[:, :] = SAMPLE_RENORM_KERNEL().fit_transform(x2.values)\n",
    "        x = x.join(x2, rsuffix='_r')\n",
    "\n",
    "    SAMPLES.append(x)\n",
    "    \n",
    "print(f'Created {len(SAMPLES)} from {len(data)} rows of data. {num_skipped} samples were skipped due to noncontinuous data')    \n",
    "\n",
    "# Create the train-test split\n",
    "ind = np.arange(len(SAMPLES))\n",
    "np.random.shuffle(ind)\n",
    "split_point = int(len(ind) * 0.7)\n",
    "\n",
    "print('Creating the training dataset...')\n",
    "TRAIN_GEN = DataGenerator(\n",
    "    [SAMPLES[i] for i in ind[:split_point]],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    feature_cols=FEATURE_COLS,\n",
    "    target_cols=TARGET_COLS\n",
    ")\n",
    "print('Creating the testing dataset...')\n",
    "TEST_GEN = DataGenerator(\n",
    "    [SAMPLES[i] for i in ind[split_point:]],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    feature_cols=FEATURE_COLS,\n",
    "    target_cols=TARGET_COLS\n",
    ")\n",
    "\n",
    "print(f'Number of training samples: {int(len(TRAIN_GEN) * 0.7)}')\n",
    "print(f'Number of testing samples:  {int(len(TEST_GEN) * 0.3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputEncodingLayer: (32, 100, 512)\n",
      "Epoch 1/100\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14967, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 21s - loss: 7.5891 - val_loss: 0.1497 - 21s/epoch - 106ms/step\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14967 to 0.08025, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 19s - loss: 3.2695 - val_loss: 0.0803 - 19s/epoch - 97ms/step\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.08025\n",
      "199/199 - 17s - loss: 4.7267 - val_loss: 7.3105 - 17s/epoch - 85ms/step\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.08025\n",
      "199/199 - 17s - loss: 6.2547 - val_loss: 0.3860 - 17s/epoch - 86ms/step\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08025 to 0.07318, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 20s - loss: 7.3406 - val_loss: 0.0732 - 20s/epoch - 98ms/step\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07318\n",
      "199/199 - 17s - loss: 7.4702 - val_loss: 0.7171 - 17s/epoch - 86ms/step\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.07318\n",
      "199/199 - 17s - loss: 7.1092 - val_loss: 0.2068 - 17s/epoch - 86ms/step\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.07318\n",
      "199/199 - 17s - loss: 6.1583 - val_loss: 0.2996 - 17s/epoch - 86ms/step\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07318\n",
      "199/199 - 17s - loss: 5.3071 - val_loss: 0.9197 - 17s/epoch - 86ms/step\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07318\n",
      "199/199 - 17s - loss: 4.2641 - val_loss: 0.2005 - 17s/epoch - 86ms/step\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.07318\n",
      "199/199 - 17s - loss: 3.5775 - val_loss: 2.2143 - 17s/epoch - 86ms/step\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.07318\n",
      "199/199 - 17s - loss: 2.9144 - val_loss: 0.4913 - 17s/epoch - 87ms/step\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.07318\n",
      "199/199 - 17s - loss: 2.3017 - val_loss: 0.1073 - 17s/epoch - 88ms/step\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.07318\n",
      "199/199 - 17s - loss: 1.8882 - val_loss: 1.2242 - 17s/epoch - 86ms/step\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.07318\n",
      "199/199 - 17s - loss: 1.5721 - val_loss: 0.2315 - 17s/epoch - 86ms/step\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.07318 to 0.06067, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 20s - loss: 1.2344 - val_loss: 0.0607 - 20s/epoch - 102ms/step\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.06067\n",
      "199/199 - 17s - loss: 1.0212 - val_loss: 0.2658 - 17s/epoch - 87ms/step\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.06067\n",
      "199/199 - 17s - loss: 0.8746 - val_loss: 0.2074 - 17s/epoch - 87ms/step\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.06067 to 0.01804, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 20s - loss: 0.7119 - val_loss: 0.0180 - 20s/epoch - 99ms/step\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01804 to 0.01419, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 20s - loss: 0.4645 - val_loss: 0.0142 - 20s/epoch - 101ms/step\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01419 to 0.01280, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 19s - loss: 0.1136 - val_loss: 0.0128 - 19s/epoch - 97ms/step\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01280\n",
      "199/199 - 17s - loss: 0.0472 - val_loss: 0.0232 - 17s/epoch - 86ms/step\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01280 to 0.01190, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 20s - loss: 0.0469 - val_loss: 0.0119 - 20s/epoch - 101ms/step\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01190 to 0.01138, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 20s - loss: 0.0467 - val_loss: 0.0114 - 20s/epoch - 99ms/step\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01138 to 0.01105, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 19s - loss: 0.0420 - val_loss: 0.0110 - 19s/epoch - 98ms/step\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01105\n",
      "199/199 - 17s - loss: 0.0326 - val_loss: 0.0212 - 17s/epoch - 86ms/step\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01105\n",
      "199/199 - 17s - loss: 0.0366 - val_loss: 0.0238 - 17s/epoch - 85ms/step\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01105\n",
      "199/199 - 17s - loss: 0.0404 - val_loss: 0.0198 - 17s/epoch - 85ms/step\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01105 to 0.00729, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 20s - loss: 0.0495 - val_loss: 0.0073 - 20s/epoch - 101ms/step\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00729 to 0.00675, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 19s - loss: 0.0410 - val_loss: 0.0068 - 19s/epoch - 97ms/step\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00675\n",
      "199/199 - 17s - loss: 0.0383 - val_loss: 0.0081 - 17s/epoch - 86ms/step\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00675 to 0.00674, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 19s - loss: 0.0447 - val_loss: 0.0067 - 19s/epoch - 97ms/step\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00674 to 0.00657, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 19s - loss: 0.0366 - val_loss: 0.0066 - 19s/epoch - 98ms/step\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00657\n",
      "199/199 - 17s - loss: 0.0384 - val_loss: 0.0336 - 17s/epoch - 87ms/step\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00657\n",
      "199/199 - 17s - loss: 0.0525 - val_loss: 0.0072 - 17s/epoch - 86ms/step\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00657\n",
      "199/199 - 17s - loss: 0.0290 - val_loss: 0.0168 - 17s/epoch - 86ms/step\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00657\n",
      "199/199 - 17s - loss: 0.0402 - val_loss: 0.0541 - 17s/epoch - 85ms/step\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00657 to 0.00600, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 20s - loss: 0.0301 - val_loss: 0.0060 - 20s/epoch - 99ms/step\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00600\n",
      "199/199 - 17s - loss: 0.0285 - val_loss: 0.0093 - 17s/epoch - 86ms/step\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00600\n",
      "199/199 - 17s - loss: 0.0226 - val_loss: 0.0080 - 17s/epoch - 85ms/step\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00600 to 0.00392, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 19s - loss: 0.0326 - val_loss: 0.0039 - 19s/epoch - 98ms/step\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00392\n",
      "199/199 - 17s - loss: 0.0235 - val_loss: 0.0190 - 17s/epoch - 85ms/step\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00392\n",
      "199/199 - 17s - loss: 0.0261 - val_loss: 0.0046 - 17s/epoch - 86ms/step\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00392\n",
      "199/199 - 17s - loss: 0.0219 - val_loss: 0.0203 - 17s/epoch - 86ms/step\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00392\n",
      "199/199 - 17s - loss: 0.0186 - val_loss: 0.0060 - 17s/epoch - 86ms/step\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00392\n",
      "199/199 - 17s - loss: 0.0188 - val_loss: 0.0088 - 17s/epoch - 85ms/step\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00392\n",
      "199/199 - 17s - loss: 0.0144 - val_loss: 0.0041 - 17s/epoch - 85ms/step\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00392\n",
      "199/199 - 17s - loss: 0.0164 - val_loss: 0.0136 - 17s/epoch - 85ms/step\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00392\n",
      "199/199 - 17s - loss: 0.0147 - val_loss: 0.0040 - 17s/epoch - 85ms/step\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00392\n",
      "199/199 - 17s - loss: 0.0137 - val_loss: 0.0059 - 17s/epoch - 86ms/step\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00392\n",
      "199/199 - 17s - loss: 0.0119 - val_loss: 0.0092 - 17s/epoch - 85ms/step\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00392\n",
      "199/199 - 17s - loss: 0.0124 - val_loss: 0.0042 - 17s/epoch - 85ms/step\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00392 to 0.00379, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 19s - loss: 0.0108 - val_loss: 0.0038 - 19s/epoch - 98ms/step\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00379\n",
      "199/199 - 17s - loss: 0.0085 - val_loss: 0.0055 - 17s/epoch - 85ms/step\n",
      "Epoch 55/100\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00379\n",
      "199/199 - 17s - loss: 0.0107 - val_loss: 0.0092 - 17s/epoch - 85ms/step\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00379\n",
      "199/199 - 17s - loss: 0.0090 - val_loss: 0.0061 - 17s/epoch - 85ms/step\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00379\n",
      "199/199 - 17s - loss: 0.0079 - val_loss: 0.0050 - 17s/epoch - 84ms/step\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00379\n",
      "199/199 - 17s - loss: 0.0083 - val_loss: 0.0074 - 17s/epoch - 85ms/step\n",
      "Epoch 59/100\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00379 to 0.00331, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 20s - loss: 0.0066 - val_loss: 0.0033 - 20s/epoch - 99ms/step\n",
      "Epoch 60/100\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00331\n",
      "199/199 - 17s - loss: 0.0067 - val_loss: 0.0041 - 17s/epoch - 85ms/step\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00331\n",
      "199/199 - 17s - loss: 0.0056 - val_loss: 0.0042 - 17s/epoch - 84ms/step\n",
      "Epoch 62/100\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00331\n",
      "199/199 - 17s - loss: 0.0054 - val_loss: 0.0041 - 17s/epoch - 84ms/step\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00331\n",
      "199/199 - 17s - loss: 0.0052 - val_loss: 0.0066 - 17s/epoch - 85ms/step\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00331\n",
      "199/199 - 17s - loss: 0.0046 - val_loss: 0.0055 - 17s/epoch - 84ms/step\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00331\n",
      "199/199 - 17s - loss: 0.0049 - val_loss: 0.0205 - 17s/epoch - 85ms/step\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00331\n",
      "199/199 - 17s - loss: 0.0042 - val_loss: 0.0047 - 17s/epoch - 85ms/step\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00331\n",
      "199/199 - 17s - loss: 0.0041 - val_loss: 0.0034 - 17s/epoch - 85ms/step\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00331\n",
      "199/199 - 17s - loss: 0.0041 - val_loss: 0.0035 - 17s/epoch - 85ms/step\n",
      "Epoch 69/100\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00331\n",
      "199/199 - 17s - loss: 0.0046 - val_loss: 0.0040 - 17s/epoch - 85ms/step\n",
      "Epoch 70/100\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00331\n",
      "199/199 - 17s - loss: 0.0042 - val_loss: 0.0036 - 17s/epoch - 85ms/step\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00331\n",
      "199/199 - 17s - loss: 0.0039 - val_loss: 0.0034 - 17s/epoch - 85ms/step\n",
      "Epoch 72/100\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00331\n",
      "199/199 - 17s - loss: 0.0042 - val_loss: 0.0034 - 17s/epoch - 85ms/step\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00331\n",
      "199/199 - 17s - loss: 0.0038 - val_loss: 0.0123 - 17s/epoch - 85ms/step\n",
      "Epoch 74/100\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00331\n",
      "199/199 - 17s - loss: 0.0042 - val_loss: 0.0089 - 17s/epoch - 85ms/step\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00331 to 0.00329, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 19s - loss: 0.0040 - val_loss: 0.0033 - 19s/epoch - 97ms/step\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00329\n",
      "199/199 - 17s - loss: 0.0039 - val_loss: 0.0075 - 17s/epoch - 85ms/step\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00329\n",
      "199/199 - 17s - loss: 0.0037 - val_loss: 0.0083 - 17s/epoch - 86ms/step\n",
      "Epoch 78/100\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00329\n",
      "199/199 - 17s - loss: 0.0038 - val_loss: 0.0062 - 17s/epoch - 85ms/step\n",
      "Epoch 79/100\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00329\n",
      "199/199 - 17s - loss: 0.0041 - val_loss: 0.0043 - 17s/epoch - 86ms/step\n",
      "Epoch 80/100\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00329\n",
      "199/199 - 17s - loss: 0.0039 - val_loss: 0.0038 - 17s/epoch - 86ms/step\n",
      "Epoch 81/100\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00329\n",
      "199/199 - 17s - loss: 0.0038 - val_loss: 0.0036 - 17s/epoch - 86ms/step\n",
      "Epoch 82/100\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00329\n",
      "199/199 - 17s - loss: 0.0045 - val_loss: 0.0150 - 17s/epoch - 86ms/step\n",
      "Epoch 83/100\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00329 to 0.00280, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 20s - loss: 0.0037 - val_loss: 0.0028 - 20s/epoch - 99ms/step\n",
      "Epoch 84/100\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00280\n",
      "199/199 - 17s - loss: 0.0041 - val_loss: 0.0041 - 17s/epoch - 86ms/step\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00280\n",
      "199/199 - 17s - loss: 0.0040 - val_loss: 0.0031 - 17s/epoch - 85ms/step\n",
      "Epoch 86/100\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00280\n",
      "199/199 - 17s - loss: 0.0039 - val_loss: 0.0055 - 17s/epoch - 86ms/step\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00280\n",
      "199/199 - 17s - loss: 0.0038 - val_loss: 0.0035 - 17s/epoch - 87ms/step\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00280\n",
      "199/199 - 17s - loss: 0.0037 - val_loss: 0.0039 - 17s/epoch - 87ms/step\n",
      "Epoch 89/100\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00280\n",
      "199/199 - 17s - loss: 0.0036 - val_loss: 0.0036 - 17s/epoch - 87ms/step\n",
      "Epoch 90/100\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00280\n",
      "199/199 - 17s - loss: 0.0037 - val_loss: 0.0062 - 17s/epoch - 87ms/step\n",
      "Epoch 91/100\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00280\n",
      "199/199 - 17s - loss: 0.0034 - val_loss: 0.0049 - 17s/epoch - 87ms/step\n",
      "Epoch 92/100\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00280\n",
      "199/199 - 17s - loss: 0.0037 - val_loss: 0.0059 - 17s/epoch - 87ms/step\n",
      "Epoch 93/100\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00280\n",
      "199/199 - 17s - loss: 0.0037 - val_loss: 0.0029 - 17s/epoch - 87ms/step\n",
      "Epoch 94/100\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00280\n",
      "199/199 - 17s - loss: 0.0037 - val_loss: 0.0032 - 17s/epoch - 86ms/step\n",
      "Epoch 95/100\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00280\n",
      "199/199 - 17s - loss: 0.0034 - val_loss: 0.0031 - 17s/epoch - 86ms/step\n",
      "Epoch 96/100\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00280 to 0.00243, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 20s - loss: 0.0039 - val_loss: 0.0024 - 20s/epoch - 103ms/step\n",
      "Epoch 97/100\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00243\n",
      "199/199 - 17s - loss: 0.0034 - val_loss: 0.0061 - 17s/epoch - 87ms/step\n",
      "Epoch 98/100\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00243 to 0.00235, saving model to model/checkpoints\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n",
      "InputEncodingLayer: (None, 100, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_6_layer_call_fn, multi_head_attention_6_layer_call_and_return_conditional_losses, FeedForwardNetworkLayer_layer_call_fn, FeedForwardNetworkLayer_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/checkpoints/assets\n",
      "WARNING:absl:<t3_model.MultiHeadAttention object at 0x7f2fac0987c0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class 't3_model.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/199 - 20s - loss: 0.0036 - val_loss: 0.0023 - 20s/epoch - 98ms/step\n",
      "Epoch 99/100\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00235\n",
      "199/199 - 17s - loss: 0.0034 - val_loss: 0.0038 - 17s/epoch - 87ms/step\n",
      "Epoch 100/100\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00235\n",
      "199/199 - 17s - loss: 0.0032 - val_loss: 0.0050 - 17s/epoch - 86ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2f947a9ee0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = T3(\n",
    "    d_model=D_MODEL,\n",
    "    dff=DFF,\n",
    "    num_heads=NUM_HEADS,\n",
    "    head_size=HEAD_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT_RATE,\n",
    "    num_outputs=len(TARGET_COLS),\n",
    "    final_activation='linear'\n",
    ")\n",
    "\n",
    "MODEL_DIR = 'model'\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "checkpoints_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f'{MODEL_DIR}/checkpoints',\n",
    "            save_weights_only=False,\n",
    "            monitor='val_loss',\n",
    "            mode='auto',\n",
    "            save_best_only=True,\n",
    "            save_freq='epoch',\n",
    "            verbose=1,\n",
    "        )\n",
    "callbacks.append(checkpoints_cb)\n",
    "\n",
    "MODEL.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer='adam',\n",
    "    )\n",
    "\n",
    "MODEL.fit(\n",
    "    x               = TRAIN_GEN,\n",
    "    epochs          = NUM_EPOCHS,\n",
    "    callbacks       = callbacks,\n",
    "    validation_data = TEST_GEN,\n",
    "    verbose         = 2,\n",
    "    shuffle         = True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
